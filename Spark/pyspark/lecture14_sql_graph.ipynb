{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f673383-3df8-4b3a-ae6c-44914f583674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                hero|          connection|\n",
      "+--------------------+--------------------+\n",
      "|             ABCISSA|ELSIE DEE,FURY, C...|\n",
      "|ABSORBING MAN | MUTA|DRAX | MUTANT X-V...|\n",
      "|ABSORBING MAN/CARL C|SOMMERS, APRIL,HE...|\n",
      "|    ADAMSON, REBECCA|KABALLA,GOLEM III...|\n",
      "|   ADVENT/KYLE GROBE|JUSTICE II/VANCE ...|\n",
      "|      AGAMEMNON III/|ASTER, LUCIAN,HOG...|\n",
      "|            AGAMOTTO|MUNIPOOR,DORMAMMU...|\n",
      "|             AGGAMON|DR. STRANGE/STEPHEN |\n",
      "|              AGINAR|SIF,REJECT/RAN-SA...|\n",
      "|                AGON|MARISTA,BLACK BOL...|\n",
      "|               AINET|STORM/ORORO MUNRO...|\n",
      "|    AKUTAGAWA, OSAMU|HUMAN TORCH/JOHNN...|\n",
      "|ALDEN, PROF. MEREDIT|CABE, BETHANY,STO...|\n",
      "|             ALISTRO|ENCHANTRESS/AMORA...|\n",
      "|       ALVAREZ, PAUL|ATOR, GENERAL,ZAR...|\n",
      "|   AMERICAN SAMURAI/|PAGE, KAREN,DARED...|\n",
      "|             AMPERE/|QUICKSILVER/PIETR...|\n",
      "|           ANCESTOR/|RECORDER II,FOUND...|\n",
      "|ANCIENT ONE/BARON MO|BLOODSTORM | MUTA...|\n",
      "|    ANDERSSEN, TANYA|KA-ZAR/KEVIN PLUN...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/jovyan/work/output already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m data\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# DataFrame.coalesce(numPartitions): Returns a new DataFrame that has exactly numPartitions partitions.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# # load the file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# csv_file_path = \"file:///home/jovyan/work/output\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# df = spark.read\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# most_popular_hero = df.select(\"hero\").first()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(most_popular_hero.hero)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/jovyan/work/output already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import (functions as f, SparkSession, types as t)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_most_popular\").getOrCreate()\n",
    "csv_file_path = \"file:///home/jovyan/work/sample/hero-network.csv\"\n",
    "\n",
    "# read file\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# pyspark.sql.functions.collect_set(col) : Aggregate function: returns a set of objects with duplicate elements eliminated\n",
    "\n",
    "data = df.groupBy(\"hero1\").agg(f.collect_set(\"hero2\").alias(\"connection\")).withColumnRenamed(\"hero1\", \"hero\")\n",
    "\n",
    "# data.show()\n",
    "# pyspark.sql.functions.concat_ws(sep, *cols): Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "data = data.withColumn(\"connection\", f.concat_ws(\",\", f.col(\"connection\")))\n",
    "data.show()\n",
    "\n",
    "# DataFrame.coalesce(numPartitions): Returns a new DataFrame that has exactly numPartitions partitions.\n",
    "data.coalesce(1).write.option(\"header\", True).csv(\"output\")\n",
    "\n",
    "\n",
    "# # load the file\n",
    "# csv_file_path = \"file:///home/jovyan/work/output\"\n",
    "# df = spark.read\\\n",
    "#             .option(\"header\", \"true\")\\\n",
    "#             .option(\"inferSchema\", \"true\")\\\n",
    "#             .csv(csv_file_path)\n",
    "# # df.show()\n",
    "\n",
    "# # pyspark.sql.functions.size(col): Collection function: returns the length of the array or map stored in the column.\n",
    "# df = df.withColumn(\n",
    "#         \"connection_size\",\n",
    "#         f.size(\n",
    "#             f.split(f.col(\"connection\"), \",\")))\\\n",
    "#         .orderBy(f.desc(\"connection_size\"))\n",
    "# df.show()\n",
    "\n",
    "# most_popular_hero = df.select(\"hero\").first()\n",
    "# print(most_popular_hero.hero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fa96f-4043-4bf1-b4ad-2cbb8bdf9799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
