{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_join\").getOrCreate()\n",
    "\n",
    "# user data\n",
    "user_data = [\n",
    "    [\"1000\", \"Neville Hardy\", \"Apple\"],\n",
    "    [\"2000\", \"Dacia Cohen\", \"Alphabet\"],\n",
    "    [\"3000\", \"Elois Cox\", \"Neflix\"],\n",
    "    [\"4000\", \"Junita Meyer\", \"Meta\"],\n",
    "    [\"5000\", \"Cleora Banks\", \"Amazon\"]]\n",
    "\n",
    "user_col = ['id', 'name', 'company']\n",
    "df_user = spark.createDataFrame(data=user_data, schema=user_col)\n",
    "df_user.show()\n",
    "\n",
    "# salary data\n",
    "salary_data = [\n",
    "    [\"1000\", \"150000\", \"engineer\"],\n",
    "    [\"2000\", \"240000\", \"manager\"],\n",
    "    [\"3000\", \"120000\", \"human resource\"],\n",
    "    [\"6000\", \"100000\", \"sales\"]]\n",
    "\n",
    "salary_col = ['id', 'salary', 'department']\n",
    "df_salary = spark.createDataFrame(data=salary_data, schema=salary_col)\n",
    "df_salary.show()\n",
    "\n",
    "# # inner join: join the two dataframes on common key columns.\n",
    "# # dataframe1.join(dataframe2,dataframe1.column_name ==  dataframe2.column_name,”inner”)\n",
    "# print(\"== inner join ==\")\n",
    "# df_user.join(df_salary,\n",
    "#                df_user.id == df_salary.id,\n",
    "#                \"inner\").show()\n",
    "\n",
    "# # inner join, then filter\n",
    "# df_user.join(df_salary,\n",
    "#                df_user.id == df_salary.id,\n",
    "#                \"inner\").filter(df_user.id == 1000).show()\n",
    "\n",
    "# # inner join, then where\n",
    "# df_user.join(df_salary,\n",
    "#                df_user.id == df_salary.id,\n",
    "#                \"inner\").where(df_user.id == 1000).show()\n",
    "\n",
    "# # multiple join with &\n",
    "# df_user.join(df_salary,\n",
    "#                (df_user.id == df_salary.id) & (df_user.id == 1000)\n",
    "#             ).show()\n",
    "\n",
    "# # full outer join: join the two dataframes with all matching and non-matching rows\n",
    "# print(\"== full outer join ==\")\n",
    "# df_user.join(df_salary, \n",
    "#                df_user.id == df_salary.id, \n",
    "#                \"fullouter\").show()\n",
    "\n",
    "# # left join:  joins by returning all rows from the first dataframe and only matched rows from the second one\n",
    "# print(\"== left join ==\")\n",
    "# df_user.join(df_salary, \n",
    "#                df_user.id == df_salary.id, \n",
    "#                \"left\").show()\n",
    "\n",
    "# # right join: joins by returning all rows from the second dataframe and only matched rows from the first one\n",
    "# print(\"== right join ==\")\n",
    "# df_user.join(df_salary, \n",
    "#                df_user.id == df_salary.id, \n",
    "#                \"right\").show()\n",
    "\n",
    "# # left semi join: join all rows from the first dataframe and return only matched rows from the second one\n",
    "# print(\"== left semi join ==\")\n",
    "# df_user.join(df_salary, \n",
    "#                df_user.id == df_salary.id, \n",
    "#                \"leftsemi\").show()\n",
    "\n",
    "# # left anti join: join returns only columns from the first dataframe for non-matched records of the second dataframe\n",
    "# print(\"== left anti join ==\")\n",
    "# df_user.join(df_salary, \n",
    "#                df_user.id == df_salary.id, \n",
    "#                \"leftanti\").show()\n",
    "\n",
    "# # SQL join\n",
    "# df_user.createOrReplaceTempView(\"user\")\n",
    "# df_salary.createOrReplaceTempView(\"salary\")\n",
    "\n",
    "# spark.sql(\"SELECT * FROM user, salary WHERE user.id == salary.id\").show()\n",
    "\n",
    "# spark.sql(\"SELECT * FROM user INNER JOIN salary ON user.id == salary.id\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
